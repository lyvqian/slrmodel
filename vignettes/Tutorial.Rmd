---
title: "Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This document introduces the usage slrmodel’s basic set of functions and compares the slrmodel functions with the `lm()` function. To learn more about the `lm()` function, please use `?lm` in R or refer to [lm( ) RDucumentation page](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm).

```{r setup}
library(slrmodel)
```

## Introduction
The simple linear regression model $Y_i={\beta_0}+{\beta_1}X_i+{\epsilon_i}$ is a model that summarizes the relationship between a predictor variable X and a response variable Y. ${\beta_0}$ and ${\beta_1}$ are usually unknown and need to be estimated using observed data (X~1~,Y~1~), (X~2~,Y~2~),…, (X~n~,Y~n~). ${\epsilon_i}'s$ are independent and identically distributed normal random variables with mean 0 and variance ${\sigma^2}$. 

The method of least squares is commonly used to find ${\hat{\beta_0}}$ and ${\hat{\beta_1}}$. Then, ${\hat{Y}}={\hat{\beta_0}}+{\hat{\beta_1}}X$ (equivalently, $Y_i={\hat{\beta_0}}+{\hat{\beta_1}}X_i+{\hat{\epsilon_i}}$) is considered as one of the best fitted models for X and Y. For more details regarding the method of least squares, please refer to the "Least Squares Method" section of [Simple Linear Regression Document](https://rpubs.com/karflisi/slr).

slrmodel aims to calculate the ${\hat{\beta_0}}$ and ${\hat{\beta_1}}$ using the method of least squares for a given set of observations of (X,Y), test the significance of (X,Y) association, and evaluate the overall fitness of the regression model $Y_i={\hat{\beta_0}}+{\hat{\beta_1}}X_i+{\epsilon_i}$.

* `slr_beta()` creates a matrix containing coefficients of the regression model. 

* `slr_rsquared()` calculates the coefficient of determination, R^2^, to evaluate the fitness of the model.

## Usage
This section shows how to use the slrmodel functions and how to interpret the results. 

### Data: iris
To explore the functions of slrmodel, we will use the dataset `iris`. This dataset contains 150 cases (rows) and 5 variables (columns). More information about the dataset can be found in `?iris`.
```{r}
dim(iris)
head(iris)
```

### Create a matrix of simple linear regression model coefficients with `slr_beta()`
`slr_beta()` allows you to generate a matrix with 2 rows and 4 columns containing ${\hat{\beta_0}}$, ${\hat{\beta_1}}$, the standard error of the two estimates, the t statistics for testing whether the estimates significantly differ from 0, and the p-values for making the statistical decision. 

The first argument `x` is a vector containing the data of the predictor variable. The second argument `y` is a vector containing the data of the predictor variable. 

For example, we can fit a simple linear regression model for the petal widths (predictor) and the petal lengths (response) of the iris flowers and obtain the coefficients of the model with:  
```{r}
slr_beta(x=iris$Petal.Width,y=iris$Petal.Length)
```
The "Estimate" of beta0 gives the value of ${\hat{\beta_0}}$, which is the intercept of the fitted linear model. It tells us the mean value of Y when X=0. Sometimes it may have an awkward interpretation. For example, in this case, we would never see petals with width=0. The "Estimate" of beta1 gives the value of ${\hat{\beta_1}}$, which is the slope of the linear regression model. It describes the mean change in Y per unit change in X. In this example, it is estimated that, the petal length would increase by 2.23 cm for 1 cm increase in the petal width.

The "Std. Error" represents the standard error of ${\hat{\beta_0}}$ and ${\hat{\beta_1}}$. 

The "t statistic" is a measure of how many standard deviations the estimate is away from 0. If the t statistic is relatively far away from 0, we would expect that the estimate is significantly different from 0. However, we need to look at p-value for more precise statistical decision.

The "p-value" is usually compared with a given significance level ${\alpha}$, say ${\alpha}=0.05$. When $p>{\alpha}$, we have insufficient evidence that the estimate is non-zero. In contrast, if $p<{\alpha}$, we can conclude that the estimate is significantly different from 0. Therefore, if the p-value for beta1 is larger than ${\alpha}$, X and Y exhibit no significant linear association. In contrast, if p-value for beta1 is smaller than ${\alpha}$, X and Y have a significant linear association. In this example, assuming ${\alpha}=0.05$, we can conclude that the petal length is significantly associated with the petal width.

To avoid awkward interpretation, we can center the covariate at its mean:
```{r}
mean_petal_width=mean(iris$Petal.Width) 
mean_petal_width

# Substract the mean petal width from each of the petal width
# Create a vector of centered petal widths
centered_petal_width=iris$Petal.Width-mean_petal_width 

# Creat a vector of petal lengths
petal_length=iris$Petal.Length

slr_beta(centered_petal_width,petal_length)
```
The intercept changes. It now tells us that the mean value of petal length is 3.758 cm when petal width is at its mean value, 1.199 cm.

Inverting the order of arguments would change the result, so you must not mix up your predictor and response variables when implementing the function, or you must state your argument with `x` and `y`:
```{r}
# The following two example are equivalent:

slr_beta(iris$Petal.Length, iris$Petal.Width)

slr_beta(y = iris$Petal.Width, x = iris$Petal.Length)
```

### Calculate the coefficient of determination with `slr_rsquared()`
`slr_rsquared()` provides the coefficient of determination R^2^. It means the proportion of the variation in Y that is explained by X. 

`slr_rsquared()` has the same arguments as `slr_beta()`. 

Again, we fit a simple linear regression model for the petal widths (predictor) and the petal lengths (response) of the iris flowers:  
```{r}
slr_rsquared(x=iris$Petal.Width,y=iris$Petal.Length)
```
92.71% of the variation in petal length is explained by the petal width.

## A comparison between functions of slrmodel and the `lm()` function
This section examines the correctness and the efficiency of the slrmodel functions.

`lm()` function is used to fit linear models, including simple linear regression models and multiple linear regression models. `lm(y~x)` models the response variable `y` as a linear function of the predictor variable `x`. 

Running `slr_beta(x,y)` is basically the same as running `summary(lm(y~x))$coef`:
```{r}
slr_beta(iris$Petal.Width,iris$Petal.Length)

summary(lm(Petal.Length~Petal.Width,data=iris))$coef

all.equal(
  # Extract and compare only the entries of the two matrices
  matrix(slr_beta(iris$Petal.Width,iris$Petal.Length),nrow=2,ncol=4,dimnames=NULL),
  matrix(summary(lm(Petal.Length~Petal.Width,data=iris))$coef,nrow=2,ncol=4,dimnames=NULL) 
)
```

Running `slr_rsquared(x,y)` is bascially the same as running `summary(lm(y~x))$r.squared`:
```{r}
slr_rsquared(iris$Petal.Width,iris$Petal.Length)

summary(lm(Petal.Length~Petal.Width,data=iris))$r.squared

all.equal(
  slr_rsquared(iris$Petal.Width,iris$Petal.Length),
  summary(lm(Petal.Length~Petal.Width,data=iris))$r.squared
)
```

`slr_beta()` was written by R code, and `slr_rsquared()` was written by C++ code. The slrmodel is like a simplified version of `lm()`, and the two functions only generate selected results of `lm()`.
In general, slrmodel functions seem to be very efficient compared to `lm()`:
```{r}
if(!require(bench)) install.packages("bench",repos = "http://cran.us.r-project.org")

bench::mark(
  slr_beta_result = matrix(
    slr_beta(iris$Petal.Width, iris$Petal.Length),
    nrow = 2,
    ncol = 4,
    dimnames = NULL
  ),
  lm_beta_result = matrix(
    summary(lm(Petal.Length ~ Petal.Width, data = iris))$coef,
    nrow = 2,
    ncol = 4,
    dimnames = NULL
  )
)

bench::mark(
  slr_r_result = slr_rsquared(iris$Petal.Width, iris$Petal.Length),
  lm_r_result = summary(lm(Petal.Length ~ Petal.Width, data = iris))$r.squared
)
```
